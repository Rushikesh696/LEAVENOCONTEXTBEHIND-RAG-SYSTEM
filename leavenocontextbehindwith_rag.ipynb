{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b15256e-1954-4441-b689-cbc7c2477dcf",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "**Build a RAG System on “Leave No Context Behind” Paper**\n",
    "\n",
    "As we know that LLMs like Gemini lack the company specific information. But this latest information is available via PDFs, Text Files, etc... Now if we can connect our LLM with these sources, we can build a much better application.\n",
    "Using LangChain framework, build a RAG system that can utilize the power of LLM like Gemini 1.5 Pro to answer questions on the “Leave No Context Behind” paper published by Google on 10th April 2024. In this process, external data(i.e. Leave No Context Behind Paper) should be retrieved and then passed to the LLM when doing the generation step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd6bcf-c7f4-4645-a68a-eccfa951fdb1",
   "metadata": {},
   "source": [
    "#### Paper Overview: \n",
    "\n",
    "**Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention**\n",
    "\n",
    "Authors: Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal (Google)\n",
    "\n",
    "Date: April 10, 2024\n",
    "\n",
    "**Key Contributions**\n",
    "\n",
    "Infini-attention mechanism\n",
    "\n",
    "A modified Transformer attention block that integrates:\n",
    "\n",
    "Compressive memory storing past key-value (KV) states instead of discarding them.\n",
    "\n",
    "Local causal attention for handling recent tokens.\n",
    "\n",
    "Long-term linear attention to retrieve compressed memory—combining both local and global context seamlessly. \n",
    "ResearchHub Storage\n",
    "arXiv\n",
    "\n",
    "Scalable and efficient long-context modeling\n",
    "\n",
    "Enables handling infinitely long inputs with fixed memory and compute costs.\n",
    "\n",
    "Achieves 114× memory compression compared to standard attention architectures. \n",
    "ResearchHub Storage\n",
    "\n",
    "Strong empirical performance\n",
    "\n",
    "A 1B-parameter model with Infini-attention manages sequence lengths up to 1M tokens and successfully completes a passkey retrieval task.\n",
    "\n",
    "An 8B-parameter model attains state-of-the-art results on a 500K-token book summarization task. \n",
    "ResearchHub Storage\n",
    "alphaXiv\n",
    "\n",
    "**Why This Matters**\n",
    "\n",
    "Extends Transformer reach: Addresses the fundamental limitation of context length in standard Transformers and LLMs.\n",
    "\n",
    "Allows real-time streaming inference on long sequences with bounded resources.\n",
    "\n",
    "Minimal architecture changes: Infini-attention is plug-and-play, meaning you can adapt existing pre-trained or finetuned models with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ceaa2-3ffb-4d97-9d05-e05640752573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b041d9f6-4d43-4cbc-9c99-17655158b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2924b9-1d41-4ff7-b431-a6310122c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct PDF URL for the paper\n",
    "url = \"https://arxiv.org/pdf/2404.07143.pdf\"  # Replace if different\n",
    "\n",
    "# Save PDF to local file\n",
    "path = Path(\"leave_no_context_behind.pdf\")\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for errors\n",
    "\n",
    "with path.open('wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8e76e0-932f-43a6-8629-956bfea17a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495619"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloaded PDF size\n",
    "path.stat().st_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d61a1e-0ad4-457f-aa77-f73f718e7fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ca4d1b-a687-4252-a07c-c8a608ce9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9c586c-daf0-4fee-b6db-9b2aa8098c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.pdf.PyPDFLoader object at 0x000001B8211C49B0>\n"
     ]
    }
   ],
   "source": [
    "# load the pdf file\n",
    "load = PyPDFLoader(\"leave_no_context_behind.pdf\")\n",
    "\n",
    "print(load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb21dcff-e288-4ba2-95e7-7fce6f91cf5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate\\nVV\\nConcat Concat\\nQ VQ VQs {KV}s\\nCompressive memory & \\nLinear attention\\nCausal scaled dot-product \\nattention & PE\\nLinear \\nprojection\\n{KV}s-1\\nRetrieve\\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1 and {KV}s are atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.\\nThe attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1\\narXiv:2404.07143v2  [cs.CL]  9 Aug 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='Preprint. Under review.\\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively\\nprocess infinitely long inputs with bounded memory footprint and computation. A key\\ncomponent in our proposed approach is a new attention technique dubbed Infini-attention\\n(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention\\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\\nattention and long-term linear attention mechanisms in a single Transformer block.\\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\\ntuning.\\nOur Infini-attention reuses all the key, value and query states of the standard attention\\ncomputation for long-term memory consolidation and retrieval. We store old KV states of\\nthe attention in the compressive memory, instead of discarding them like in the standard\\nattention mechanism. We then retrieve the values from the memory by using the attention\\nquery states when processing subsequent sequences. To compute the final contextual\\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\\nattention contexts.\\nIn our experiments, we show that our approach outperforms baseline models on long-\\ncontext language modeling benchmarks while having 114x comprehension ratio in terms of\\nmemory size. The model achieves even better perplexity when trained with 100K sequence\\nlength. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval\\ntask when injected with Infini-attention. Finally, we show that a 8B model with Infini-\\nattention reaches a new SOTA result on a 500K length book summarization task after\\ncontinual pre-training and task fine-tuning.\\nIn summary, our work makes the following contributions:\\n1. We introduce a practical and yet powerful attention mechanism – Infini-attention\\nwith long-term compressive memory and local causal attention for efficiently mod-\\neling both long and short-range contextual dependencies.\\n2. Infini-attention introduces minimal change to the standard scaled dot-product atten-\\ntion and supports plug-and-play continual pre-training and long-context adaptation\\nby design.\\n3. Our approach enables Transformer LLMs to scale to infinitely long context with a\\nbounded memory and compute resource by processing extremely long inputs in a\\nstreaming fashion.\\n2 Background\\nRecurrent Neural Networks (RNNs) process a single token xt at each step t and computes a\\nrecurrent hidden state ht to represent an entire input sequence (Hochreiter & Schmidhuber,\\n1997; Maass et al., 2002):\\nht = RNN (xt, ht−1). (1)\\nThe RNN computation is very efficient since the model maintains only a fixed-size vector\\nht for input sequence. However, for processing long sequences it becomes difficult to\\nstore entire contextual information into a single fixed-size vector and this limitation had\\nimplications on RNNs utility in certain tasks (Kaiser & Sutskever, 2015). To address the\\nlimitation, people extended the standard RNNs with an external memory component that\\ncan be read from and written to. One such an instance is Metalearned Neural Memory\\n(MNM) (Munkhdalai et al., 2019):\\nht, θt = MNM (xt, ht−1, θt−1). (2)\\nMNM learns an additional memory state θ parameterized by a feed-forward neural network\\n(FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar\\nto the attention mechanism. To store information, it modifies the parameters of the FFN\\nby using the key vectors as input and the value vectors for the target, and to read memory\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='Preprint. Under review.\\nSegment 1 Segment 2 Segment 3\\nSegment 1 Segment 2 Segment 3\\nTransformer block:\\nInfini-Transformer\\nTransformer-XL\\nCompressive memory:\\nMemory update:\\nMemory retrieval:\\nEffective context:\\nInput segment: Segment 1\\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\\n(bottom) discards old contexts since it caches the KV states for the last segment only.\\nentries, it forward-passes the query vectors through the memory FFN and retrieves its\\ncorresponding value. Like RNNs, the memory state is still bounded in MNM.\\nUnlike the RNNs, the attention mechanism however doesn’t maintain a recurrent state and\\nonly performs a feed-forward computation on input sequence segment Xs:\\nOs = attention (Xs). (3)\\nThe attention output Os is simply passed to the next layer and no state is carried over\\nto the next input sequence Xs+1 at the same attention layer . In the attention layer, in\\norder to capture the dependency between the consequent segments Xs and Xs+1, one\\nneeds to process them altogether at the same time and this process becomes a bottleneck\\nrequiring large computational resources as the length of input sequence grows more and\\nmore. To improve the efficiency while still being able to benefit from the expressiveness of\\nthe attention mechanism, this work introduces a recurrent attention layer.\\n3 Method\\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).\\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We\\ncompute the standard causal dot-product attention context within each segment. So the\\ndot-product attention computation is local in a sense that it covers a total N number of\\ntokens of the current segment with index S (N is the segment length).\\nThe local attention (Dai et al., 2019), however, discards the attention states of the previous\\nsegment when processing the next one. In Infini-Transformers, instead of leaving out the\\nold KV attention states, we propose to reuse them to maintain the entire context history\\nwith a compressive memory. So each attention layer of Infini-Transformers has both global\\ncompressive and local fine-grained states. We call such an efficient attention mechanism\\nInfini-attention, which is illustrated in Figure 1 and described formally in the following\\nsections.\\n3.1 Infini-attention\\nAs shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes\\nboth local and global context states and combine them for its output. Similar to multi-head\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='Preprint. Under review.\\nattention (MHA), it maintains H number of parallel compressive memory per attention\\nlayer (H is the number of attention heads) in addition to the dot-product attention and like\\nthe RNNs and MNM, it maintains a recurrent memory state to efficiently track the long\\nsequence context:\\nOs, Ms = in f ini-attention (Xs, Ms−1) (4)\\n3.1.1 Scaled Dot-product Attention\\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\\nLLMs. The MHA’s strong capability to model context-dependent dynamic computation and\\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\\ngenerative models.\\nA single head in the vanilla MHA computes its attention context Adot ∈ I RN×dvalue from\\nsequence of input segments X ∈ I RN×dmodel as follows. First, it computes attention query,\\nkey, and value states:\\nK = XWK, V = XWV and Q = XWQ. (5)\\nHere, WK ∈ I Rdmodel ×dkey , WV ∈ I Rdmodel ×dvalue and WQ ∈ I Rdmodel ×dkey are trainable projection\\nmatrices. Then, the attention context is calculated as a weighted average of all other values\\nas\\nAdot = softmax\\n\\x12 QKT\\n√dmodel\\n\\x13\\nV. (6)\\nFor MHA, we compute H number of attention context vectors for each sequence element\\nin parallel, concatenate them along the second dimension and then finally project the\\nconcatenated vector to the model space to obtain the attention output.\\n3.1.2 Compressive Memory\\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\\nreuse the query, key and value states (Q, K and V) from the dot-product attention compu-\\ntation. The state sharing and reusing between the dot-product attention and compressive\\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up\\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\\nstore bindings of key and value states in the compressive memory and retrieve by using the\\nquery vectors.\\nWhile there are different forms of compressive memory proposed in the literature (Hop-\\nfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and\\ncomputational efficiency, in this work we parameterize the memory with an associative\\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\\nstable training techniques from the related methods. Specially, we adopt the update rule\\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\\ncompetitive performance.\\nMemory retrieval. In Infini-attention, we retrieve new content Amem ∈ I RN×dvalue from the\\nmemory Ms−1 ∈ I Rdkey ×dvalue by using the query Q ∈ I RN×dkey as:\\nAmem = σ(Q)Ms−1\\nσ(Q)zs−1\\n. (7)\\nHere, σ and zs−1 ∈ I Rdkey are a nonlinear activation function and a normalization term,\\nrespectively. As the choice of the non-linearity and the norm method is crucial for training\\nstability, following Katharopoulos et al. (2020) we record a sum over all keys as the normal-\\nization term zs−1 and use element-wise ELU + 1 as the activation function (Clevert et al.,\\n2015).\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='Preprint. Under review.\\nModel Memory (cache) footprint Context length Memory update Memory retrieval\\nTransformer-XL (dkey + dvalue ) × H × N × l N × l Discarded Dot-product attention\\nCompressive Transformer dmodel × (c + N) × l (c × r + N) × l Discarded Dot-product attention\\nMemorizing Transformers (dkey + dvalue ) × H × N × S N × S None kNN + dot-product attention\\nRMT dmodel × p × l × 2 N × S Discarded Soft-prompt input\\nAutoCompressors dmodel × p × (m + 1) × l N × S Discarded Soft-prompt input\\nInfini-Transformers dkey × (dvalue + 1) × H × l N × S Incremental Linear attention\\nTable 1: Transformer models with segment-level memory are compared. For each model, the\\nmemory size and effective context length are defined in terms of their model parameters (N:\\ninput segment length, S: the number of segments, l: the number of layers, H: the number\\nof attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the\\nnumber of soft-prompt summary vectors and m: summary vector accumulation steps).\\nMemory update. Once the retrieval is done, we update the memory and the normalization\\nterm with the new KV entries and obtain the next states as\\nMs ← Ms−1 + σ(K)TV and zs ← zs−1 +\\nN\\n∑\\nt=1\\nσ(Kt). (8)\\nThe new memory states Ms and zs are then passed to the next segment S + 1, building in\\na recurrence in each attention layer. The right side term σ(K)TV in Eq. (8) is known as an\\nassociative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),\\nwe have also incorporated it into our Infini-attention. The delta rule attempts a slightly\\nimproved memory update by first retrieving existing value entries and subtracting them\\nfrom the new values before applying the associative bindings as new update.\\nMs ← Ms−1 + σ(K)T(V − σ(K)Ms−1\\nσ(K)zs−1\\n). (9)\\nThis update rule (Linear + Delta ) leaves the associative matrix unmodified if the KV binding\\nalready exists in the memory while still tracking the same normalization term as the former\\none (Linear ) for numerical stability.\\nLong-term context injection. We aggregate the local attention state Adot and memory\\nretrieved content Amem via a learned gating scalar β:\\nA = sigmoid(β) ⊙ Amem + (1 − sigmoid(β)) ⊙ Adot . (10)\\nThis adds only a single scalar value as training parameter per head while allowing a\\nlearnable trade-off between the long-term and local information flows in the model (Wu\\net al., 2022).\\nSimilar to the standard MHA, for the multi-head Infini-attention we compute H number of\\ncontext states in parallel, and concatenate and project them for the final attention output\\nO ∈ I RN×dmodel :\\nO = [A1; . . .AH]WO (11)\\nwhere WO ∈ I RH×dvalue ×dmodel is trainable weights.\\n3.2 Memory and Effective Context Window\\nOur Infini-Transformer enables an unbounded context window with a bounded memory\\nfootprint. To illustrate this, Table 1 lists the previous segment-level memory models with\\ntheir context-memory footprint and effective context length defined in terms of model\\nparameters and input segment length. Infini-Transformer has a constant memory complexity\\nof dkey × dvalue + dkey for storing compressed context in Ms and zs for each head in single\\nlayer while for the other models, the complexity grows along with the sequence dimension\\n- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,\\n2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,\\n2022) or on the soft-prompt size for RMT (Bulatov et al., 2022) and AutoCompressors (Ge\\net al., 2023).\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='Preprint. Under review.\\nEarly \\nlayers\\nAttention heads\\nFigure 3: There are two types of heads\\nemerged in Infini-attention after training: spe-\\ncialized heads with gating score near 0 or\\n1 and mixer heads with score close to 0.5.\\nThe specialized heads either process contex-\\ntual information via the local attention mech-\\nanism or retrieve from the compressive mem-\\nory whereas the mixer heads aggregate both\\ncurrent contextual information and long-term\\nmemory content together into single output.\\nTransformer-XL computes attention over KV\\nstates cached from the last segment in addition\\nto the current states. Since this is done for each\\nlayer, Transformer-XL extends the context win-\\ndow from N to N × l tokens with an additional\\nmemory footprint of (dkey + dvalue ) × H × N × l.\\nCompressive Transformer adds a second cache\\nto Transformer-XL and stores compressed rep-\\nresentations of past segment activations. So it\\nextends the Transformer-XL’s context window\\nby c × r × l but still has a large context-memory\\ncomplexity. Taking the idea further, Memoriz-\\ning Transformers opt to store the entire KV states\\nas context for input sequences. Since the stor-\\nage becomes prohibitively expensive in this case,\\nthey restrict the contextual computation to a sin-\\ngle layer only. By utilizing a fast kNN retriever,\\nMemorizing Transformers then build a context\\nwindow covering the entire sequence history of\\nlength N × S at an increased cost of storage. Our\\nexperiments show that Infini-Transformer LM\\ncan achieve more than 100x compression rate on\\ntop of Memorizing Transformers while further\\nimproving the perplexity score.\\nRMT and AutoCompressors allow for a poten-\\ntially infinite context length since they compress\\nthe input into summary vectors and then pass them as extra soft-prompt inputs for the subse-\\nquent segments. However, in practice the success of those techniques highly depends on the\\nsize of soft-prompt vectors. Namely, it is necessary to increase the number of soft-prompt\\n(summary) vectors to achieve a better performance with AutoCompressors (Chevalier\\net al., 2023) and with that, the memory and compute complexity grow quickly resulting\\nin diminished efficiency. It was also observed in AutoCompressors (Chevalier et al., 2023)\\nthat an efficient compression objective is needed for training such prompt compression\\ntechniques (Ge et al., 2023).\\n4 Experiments\\nWe evaluated our Infini-Transformer models on benchmarks involving extremely long input\\nsequences: long-context language modeling, 1M length passkey context block retrieval\\nand 500K length book summarization tasks. For the language modeling benchmark, we\\ntrain our models from scratch while for the passkey and book summarization tasks, we\\ncontinually pre-train existing LLMs in order to highlight a plug-and-play long-context\\nadaptation capability of our approach.\\n4.1 Implementation details\\nSegment chunking. we forward-pass the entire input text a Transformer model and then\\nperform segment chunking at each Infini-attention layer - in this way, perform a minimal\\nmodification to the existing Transformer implementation. The Infini-attention layer seg-\\nments the input and process it segment by segment and concatenates back the segments to\\npass the original-length segment as output to the next layer.\\nBack-propagation through time (BPTT). Each Infini-attention layer is trained with back-\\npropagation through time (Werbos, 1988) by computing the gradient w.r.t the compressive\\nmemory states, similar to how RNNs are trained. To save memory, we perform gradient\\ncheckpoint when processing the sequence segment by segment.\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='Preprint. Under review.\\nModel Memory size (comp.) XL cache Segment length PG19 Arxiv-math\\nTransformer-XL 50M (3.7x) 2048 2048 11.88 2.42\\nMemorizing Transformers 183M (1x) 2048 2048 11.37 2.26\\nRMT 2.5M (73x) None 2048 13.27 2.55\\nInfini-Transformer(Linear) 1.6M (114x) None 2048 9.65 2.24\\nInfini-Transformer(Linear + Delta) 1.6M (114x) None 2048 9.67 2.23\\nTable 2: Long-context language modeling results are compared in terms of average token-\\nlevel perplexity. Comp. denotes compression ratio. Infini-Transformer outperforms memo-\\nrizing transformers with memory length of 65K and achieves 114x compression ratio.\\nPosition Embeddings (PE). As shown Figure 1, we don’t use position embeddings for\\nthe key and query vectors of the compressive memory to store only global contextual\\ninformation in the long-term memory. The PEs were applied to the QK vectors only after\\nthe compressive memory reading and update.\\n4.2 Long-context Language Modeling\\nWe trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019) and\\nArxiv-math (Wu et al., 2022) benchmarks. Our setup closely resembles that of Memorizing\\nTransformers (Wu et al., 2022). Namely, all our models have 12 layers and 8 attention heads\\nof dimension 128 each and FFNs with hidden layer 4096.\\nWe set the Infini-attention segment length N to 2048 for all attention layers and the input\\nsequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps\\nw.r.t its compressive memory states. For the RMT baseline, we performed several runs with\\nsummary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT\\nwith 100 summary vectors gave the best result when trained on 8196 length sequences.\\nThe main results from the language modeling experiments are summarized in Table 2. Our\\nInfini-Transformer outperforms both Transformer-XL (Dai et al., 2019) and Memorizing\\nTransformers (Wu et al., 2022) baselines while maintaining 114x less memory parameters\\nthan the Memorizing Transformer model with a vector retrieval-based KV memory with\\nlength of 65K at its 9th layer.\\n100K length training. We further increased the training sequence length to 100K from\\n32K and trained the models on Arxiv-math dataset. 100K training further decreased the\\nperplexity score to 2.21 and 2.20 for Linear and Linear + Delta models.\\nGating score visualization. Figure 3 visualizes the gating score, sigmoid(β) for the compres-\\nsive memory for all attention heads in each layer. There are two types of heads emerged in\\nInfini-attention after training: specialized heads with a gating score near 0 or 1 and mixer\\nheads with a score close to 0.5. The specialized heads either process contextual information\\nvia the local attention computation or retrieve from the compressive memory whereas the\\nmixer heads aggregate both current contextual information and long-term memory content\\ntogether into a single output. Interestingly, each layer has at least a single short-range\\nhead, allowing a forward-propagation of input signal up until the output layer. We also\\nZero-shot\\n32K 128K 256K 512K 1M\\nInfini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98\\nInfini-Transformer(Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97\\nFT (400 steps)\\nInfini-Transformer(Linear) 100/100/100 100/100/100 100/100/100 97/99/100 96/94/100\\nInfini-Transformer(Linear + Delta) 100/100/100 100/100/99 100/100/99 100/100/100 100/100/100\\nTable 3: Infini-Transformers solved the passkey task with up to 1M context length when\\nfine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden\\nin a different part (start/middle/end) of long inputs with lengths 32K to 1M.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='Preprint. Under review.\\nModel Rouge-1 Rouge-2 Rouge-L Overall\\nBART 36.4 7.6 15.3 16.2\\nBART + Unlimiformer 36.8 8.3 15.7 16.9\\nPRIMERA 38.6 7.2 15.6 16.3\\nPRIMERA + Unlimiformer 37.9 8.2 16.3 17.2\\nInfini-Transformers(Linear) 37.9 8.7 17.6 18.0\\nInfini-Transformers(Linear + Delta) 40.0 8.8 17.9 18.5\\nTable 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and\\nUnlimiformer results are from Bertsch et al. (2024).\\nobserved an interleaving of long and short-term content retrievals throughout the forward\\ncomputation.\\n4.3 LLM Continual Pre-training\\nWe performed a lightweight continual pre-training for long-context adaptation of existing\\nLLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4\\ntext (Raffel et al., 2020) with length more than 4K tokens. The segment length N was set to\\n2K throughout our experiments.\\n1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-\\nattention and continued to pre-train on inputs with length of 4K. The model was trained for\\n30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami\\n& Jaggi, 2024).\\nThe passkey task hides a random number into a long text and asks it back at the model\\noutput. The length of the distraction text is varied by repeating a text chunk multiple times.\\nThe previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up\\nto 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.\\nWe take this challenge further and fine-tune on only 5K length inputs to test on 1M length\\nregime.\\nInput length\\nRouge overall score\\n17\\n18\\n19\\n20\\n16K 32K 64K 128K 256K 500K\\nFigure 4: Infini-Transformers obtain better\\nRouge overall scores with more book text pro-\\nvided as input.\\nTable 3 reports the token-level accuracy for\\ntest subsets with input lengths ranging from\\n32K to 1M. For each test subset, we con-\\ntrolled the position of the passkey so that it\\nis either located around the beginning, mid-\\ndle or the end of the input sequence. We\\nreported both zero-shot accuracy and fine-\\ntuning accuracy. Infini-Transformers solved\\nthe task with up to 1M context length af-\\nter fine-tuning on 5K length inputs for 400\\nsteps.\\n500K length book summarization (Book-\\nSum). We further scaled our approach by\\ncontinuously pre-training a 8B LLM model\\nwith 8K input length for 30K steps. We then\\nfine-tuned on a book summarization task,\\nBookSum (Kry´sci ´nski et al., 2021) where the\\ngoal is to generate a summary of an entire\\nbook text.\\nWe set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a\\ngeneration temperature of 0.5 and top p = 0.95 and set the number of decoding steps to 1024\\nto generate a summary of each book.\\nTable 4 compares our model against the encoder-decoder models that were built particularly\\nfor the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based\\nlong-context extension (Bertsch et al., 2024). Our model outperforms the previous best\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='Preprint. Under review.\\nresults and achieves a new SOTA on BookSum by processing the entire text from book. We\\nhave also plotted the overall Rouge score on validation split of BookSum data in Figure 4.\\nThere is a clear trend showing that with more text provided as input from books, Our\\nInfini-Transformers improves its summarization performance metric.\\n5 Related Work\\nCompressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu,\\n2017a; Miconi et al., 2018), compressive memory approaches cast parameterized functions\\nas memory to store and retrieve information (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba\\net al., 2016; Munkhdalai et al., 2019). Unlike the Transformer KV memory array (Vaswani\\net al., 2017; Wu et al., 2022), which grows with input sequence length, compressive memory\\nsystems maintain a constant number of memory parameters for computational efficiency.\\nThe parameters are modified with an update rule to store information, which is then\\nretrieved via a memory reading mechanism (Graves et al., 2014; Sukhbaatar et al., 2015;\\nMunkhdalai & Yu, 2017b).\\nCompressed input representations can be viewed as a summary of past sequence seg-\\nments (Rae et al., 2019; Chevalier et al., 2023). Along this direction, more recent works\\nhave been utilizing a Transformer LLM itself to compress input sequence for efficient long-\\ncontext modeling (Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2023; Mu et al., 2024;\\nHwang et al., 2024). However, the previous segment-level compression methods, including\\nCompressive Transformers (Rae et al., 2019) still discard the memory entries of old segments\\nin order to free up space for the new ones, limiting their context window to the most recent\\nsegments. This is in contrast to our Infini-attention that computes incremental memory\\nupdates to a fixed amount of memory parameters in a recurrent fashion.\\nLong-context continual pre-training. There is a line of work that extends the dot-product\\nattention layers and continues to train LLMs for long-context (Xiong et al., 2023; Fu et al.,\\n2024). The attention extensions include incorporating sparsity into the attention layer (Chen\\net al., 2023b; Ratner et al., 2022; Mohtashami & Jaggi, 2024) as well as manipulating the\\nposition encodings (Chen et al., 2023a; Peng et al., 2023). Although the position encoding-\\nbased methods such as position interpolation techniques (Chen et al., 2023a) can be data\\nefficient as they only adjust the positional bias in the attention layer, they are still costly for\\ninference.\\nThe attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and\\nlost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context\\nlength is longer than what was observed during training (Press et al., 2021; Kazemnejad\\net al., 2024). The proposed Infini-attention addresses those issues by enabling a segment-\\nlevel streaming computation over long sequences with a fixed local attention window. Our\\nInfini-Transformers successfully extrapolate to 1M input length regimes when trained on\\n32K and even 5K length sequences.\\nEfficient attention. The efficient attention techniques attempt to improve the efficiency of\\nthe dot-product attention with an approximation or a system-level optimization. Multiple\\ndirections have been explored for different forms of efficient attention approximation,\\nincluding sparsity-based (Child et al., 2019; Beltagy et al., 2020; Sukhbaatar et al., 2021;\\nDing et al., 2023; Xiao et al., 2024) and linear attention approximation (Shen et al., 2018;\\nKatharopoulos et al., 2020; Schlag et al., 2021). Among those, the linear attention variants\\nare closely related to the associative memory matrix (Schlag et al., 2020; 2021) and the\\nmetalearned neural memory (Munkhdalai et al., 2019), where KV bindings (Smolensky,\\n1990) are stored in Fast-Weights (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016)\\nthat are modified in with respect to new contextual information. More recently, system-level\\noptimization techniques have been proposed by leveraging specific hardware architecture\\nto make the exact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='Preprint. Under review.\\n6 Conclusion\\nAn effective memory system is crucial not just for comprehending long contexts with LLMs,\\nbut also for reasoning, planning, continual adaptation for fresh knowledge, and even for\\nlearning how to learn. This work introduces a close integration of compressive memory mod-\\nule into the vanilla dot-product attention layer. This subtle but critical modification to the\\nattention layer enables LLMs to process infinitely long contexts with bounded memory and\\ncomputation resources. We show that our approach can naturally scale to a million length\\nregime of input sequences, while outperforming the baselines on long-context language\\nmodeling benchmark and book summarization tasks. We also demonstrate a promising\\nlength generalization capability of our approach. 1B model that was fine-tuned on up to 5K\\nsequence length passkey instances solved the 1M length problem.\\nAcknowledgments\\nWe would like to thank Dongseong Hwang for their help implementing efficient sequence\\nunrolling mechanism with the jax scan function. We would also like to thank Aditya Gupta,\\nKalpesh Krishna, Tu Vu and Alexandra Chronopoulou for their feedback.\\nReferences\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using\\nfast weights to attend to the recent past. Advances in neural information processing systems,\\n29, 2016.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\\nformer. arXiv preprint arXiv:2004.05150, 2020.\\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-\\nrange transformers with unlimited length input. Advances in Neural Information Processing\\nSystems, 36, 2024.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\\nLanguage models are few-shot learners. Advances in neural information processing systems,\\n33:1877–1901, 2020.\\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.Advances\\nin Neural Information Processing Systems, 35:11079–11091, 2022.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending con-\\ntext window of large language models via positional interpolation. arXiv preprint\\narXiv:2306.15595, 2023a.\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\\nLonglora: Efficient fine-tuning of long-context large language models. arXiv preprint\\narXiv:2309.12307, 2023b.\\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\\nmachine reading. arXiv preprint arXiv:1601.06733, 2016.\\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language\\nmodels to compress contexts. arXiv preprint arXiv:2305.14788, 2023.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='Preprint. Under review.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\nDjork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep\\nnetwork learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv\\npreprint arXiv:1901.02860, 2019.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast\\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\\nProcessing Systems, 35:16344–16359, 2022.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,\\nNanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.\\narXiv preprint arXiv:2307.02486, 2023.\\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\\narXiv:2402.10171, 2024.\\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945, 2023.\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\\narXiv:1410.5401, 2014.\\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-\\nating the science of language models. arXiv preprint arXiv:2402.00838, 2024.\\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology\\npress, 2005.\\nGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In\\nProceedings of the ninth annual conference of the Cognitive Science Society, pp. 177–186, 1987.\\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9\\n(8):1735–1780, 1997.\\nJohn J Hopfield. Neural networks and physical systems with emergent collective computa-\\ntional abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.\\nDongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno\\nMengibar. Transformerfam: Feedback attention is working memory. arXiv preprint\\narXiv:2404.09173, 2024.\\nŁukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint\\narXiv:1511.08228, 2015.\\nPentti Kanerva. Sparse distributed memory. MIT press, 1988.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers\\nare rnns: Fast autoregressive transformers with linear attention. In International conference\\non machine learning, pp. 5156–5165. PMLR, 2020.\\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and\\nSiva Reddy. The impact of positional encoding on length generalization in transformers.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\nWojciech Kry´sci´nski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir\\nRadev. Booksum: A collection of datasets for long-form narrative summarization. arXiv\\npreprint arXiv:2105.08209, 2021.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='Preprint. Under review.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv\\npreprint arXiv:1910.13461, 2019.\\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for\\nnear-infinite context. arXiv preprint arXiv:2310.01889, 2023.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language models use long contexts.Transactions\\nof the Association for Computational Linguistics, 12:157–173, 2024.\\nWolfgang Maass, Thomas Natschl¨ager, and Henry Markram. Real-time computing without\\nstable states: A new framework for neural computation based on perturbations. Neural\\ncomputation, 14(11):2531–2560, 2002.\\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\\nneural networks with backpropagation. In International Conference on Machine Learning,\\npp. 3559–3568. PMLR, 2018.\\nAmirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for\\ntransformers. Advances in Neural Information Processing Systems, 36, 2024.\\nJesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine\\nlearning, pp. 2554–2563. PMLR, 2017a.\\nTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In Proceedings of the\\nconference. Association for Computational Linguistics. Meeting, volume 1, pp. 397. NIH Public\\nAccess, 2017b.\\nTsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention\\nmodels. In Proceedings of the Seventh International Workshop on Health Text Mining and\\nInformation Analysis, pp. 69–77, 2016.\\nTsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned\\nneural memory. Advances in Neural Information Processing Systems, 32, 2019.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\\nwindow extension of large language models. arXiv preprint arXiv:2309.00071, 2023.\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,\\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-\\nformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear\\nbiases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\\nunified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551,\\n2020.\\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon\\nShashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve\\nin-context learning of large language models. arXiv preprint arXiv:2212.10947, 2022.\\nImanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, J ¨urgen Schmidhuber,\\nand Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math\\nproblem solving. arXiv preprint arXiv:1910.06611, 2019.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Preprint. Under review.\\nImanol Schlag, Tsendsuren Munkhdalai, and J ¨urgen Schmidhuber. Learning associative\\ninference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020.\\nImanol Schlag, Kazuki Irie, and J ¨urgen Schmidhuber. Linear transformers are secretly\\nfast weight programmers. In International Conference on Machine Learning, pp. 9355–9366.\\nPMLR, 2021.\\nJ ¨urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic\\nrecurrent networks. Neural Computation, 4(1):131–139, 1992.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear\\nmemory cost. In International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.\\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient\\nattention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018.\\nPaul Smolensky. Tensor product variable binding and the representation of symbolic\\nstructures in connectionist systems. Artificial intelligence, 46(1-2):159–216, 1990.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.\\nAdvances in neural information processing systems, 28, 2015.\\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,\\nand Angela Fan. Not all memories are created equal: Learning to forget by expiring. In\\nInternational Conference on Machine Learning, pp. 9902–9912. PMLR, 2021.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems, 30, 2017.\\nPaul J Werbos. Generalization of backpropagation with application to a recurrent gas market\\nmodel. Neural networks, 1(4):339–356, 1988.\\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing\\ntransformers. arXiv preprint arXiv:2203.08913, 2022.\\nChaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang,\\nZhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity\\nof llms for understanding extremely long sequences with training-free memory. arXiv\\npreprint arXiv:2402.04617, 2024.\\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient stream-\\ning language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramid-\\nbased masked sentence pre-training for multi-document summarization. arXiv preprint\\narXiv:2110.08499, 2021.\\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective\\nlong-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.\\nA Additional Training Details\\nFor the long-context language modeling task, we set the learning rate to 0.01 by perform-\\ning small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor opti-\\nmizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine\\ndecay. We applied gradient checkpointing after each segment to save to save memory. The\\nbatch size was set to 64. For the LLM experiments, we set the learning rate to 0.0001 during\\ncontinual pre-training and task fine-tuning.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='Preprint. Under review.\\nB Passkey Retrieval Task\\nBelow we showed the input format of the passkey task.\\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I\\nwill quiz you about the important information there. The grass is green. The sky is blue. The sun\\nis yellow. Here we go. There and back again.(repeat x times)The pass key is9054. Remember\\nit. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.\\nThere and ack again. (repeat y times)What is the pass key? The pass key is\\n14')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = load.load()\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682c7b18-5adc-4dfc-b293-39c1a4425b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001B8235BFD70>\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "print(splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89b17719-5a8e-4f26-b907-3ef59ec51d53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='memory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate\\nVV\\nConcat Concat\\nQ VQ VQs {KV}s\\nCompressive memory & \\nLinear attention\\nCausal scaled dot-product \\nattention & PE\\nLinear \\nprojection\\n{KV}s-1\\nRetrieve\\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1 and {KV}s are atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.\\nThe attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='embeddings.\\nThe attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='ory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1\\narXiv:2404.07143v2  [cs.CL]  9 Aug 2024'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='Preprint. Under review.\\nIn this work, we introduce a novel approach that enables Transformer LLMs to effectively\\nprocess infinitely long inputs with bounded memory footprint and computation. A key\\ncomponent in our proposed approach is a new attention technique dubbed Infini-attention\\n(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention\\nmechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local\\nattention and long-term linear attention mechanisms in a single Transformer block.\\nSuch a subtle but critical modification to the Transformer attention layer enables a natural\\nextension of existing LLMs to infinitely long contexts via continual pre-training and fine-\\ntuning.\\nOur Infini-attention reuses all the key, value and query states of the standard attention\\ncomputation for long-term memory consolidation and retrieval. We store old KV states of'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='computation for long-term memory consolidation and retrieval. We store old KV states of\\nthe attention in the compressive memory, instead of discarding them like in the standard\\nattention mechanism. We then retrieve the values from the memory by using the attention\\nquery states when processing subsequent sequences. To compute the final contextual\\noutput, the Infini-attention aggregates the long-term memory-retrieved values and the local\\nattention contexts.\\nIn our experiments, we show that our approach outperforms baseline models on long-\\ncontext language modeling benchmarks while having 114x comprehension ratio in terms of\\nmemory size. The model achieves even better perplexity when trained with 100K sequence\\nlength. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval\\ntask when injected with Infini-attention. Finally, we show that a 8B model with Infini-\\nattention reaches a new SOTA result on a 500K length book summarization task after'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='attention reaches a new SOTA result on a 500K length book summarization task after\\ncontinual pre-training and task fine-tuning.\\nIn summary, our work makes the following contributions:\\n1. We introduce a practical and yet powerful attention mechanism – Infini-attention\\nwith long-term compressive memory and local causal attention for efficiently mod-\\neling both long and short-range contextual dependencies.\\n2. Infini-attention introduces minimal change to the standard scaled dot-product atten-\\ntion and supports plug-and-play continual pre-training and long-context adaptation\\nby design.\\n3. Our approach enables Transformer LLMs to scale to infinitely long context with a\\nbounded memory and compute resource by processing extremely long inputs in a\\nstreaming fashion.\\n2 Background\\nRecurrent Neural Networks (RNNs) process a single token xt at each step t and computes a\\nrecurrent hidden state ht to represent an entire input sequence (Hochreiter & Schmidhuber,\\n1997; Maass et al., 2002):'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='1997; Maass et al., 2002):\\nht = RNN (xt, ht−1). (1)\\nThe RNN computation is very efficient since the model maintains only a fixed-size vector\\nht for input sequence. However, for processing long sequences it becomes difficult to\\nstore entire contextual information into a single fixed-size vector and this limitation had\\nimplications on RNNs utility in certain tasks (Kaiser & Sutskever, 2015). To address the\\nlimitation, people extended the standard RNNs with an external memory component that\\ncan be read from and written to. One such an instance is Metalearned Neural Memory\\n(MNM) (Munkhdalai et al., 2019):\\nht, θt = MNM (xt, ht−1, θt−1). (2)\\nMNM learns an additional memory state θ parameterized by a feed-forward neural network\\n(FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar\\nto the attention mechanism. To store information, it modifies the parameters of the FFN\\nby using the key vectors as input and the value vectors for the target, and to read memory\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='Preprint. Under review.\\nSegment 1 Segment 2 Segment 3\\nSegment 1 Segment 2 Segment 3\\nTransformer block:\\nInfini-Transformer\\nTransformer-XL\\nCompressive memory:\\nMemory update:\\nMemory retrieval:\\nEffective context:\\nInput segment: Segment 1\\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL\\n(bottom) discards old contexts since it caches the KV states for the last segment only.\\nentries, it forward-passes the query vectors through the memory FFN and retrieves its\\ncorresponding value. Like RNNs, the memory state is still bounded in MNM.\\nUnlike the RNNs, the attention mechanism however doesn’t maintain a recurrent state and\\nonly performs a feed-forward computation on input sequence segment Xs:\\nOs = attention (Xs). (3)\\nThe attention output Os is simply passed to the next layer and no state is carried over\\nto the next input sequence Xs+1 at the same attention layer . In the attention layer, in'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='to the next input sequence Xs+1 at the same attention layer . In the attention layer, in\\norder to capture the dependency between the consequent segments Xs and Xs+1, one\\nneeds to process them altogether at the same time and this process becomes a bottleneck\\nrequiring large computational resources as the length of input sequence grows more and\\nmore. To improve the efficiency while still being able to benefit from the expressiveness of\\nthe attention mechanism, this work introduces a recurrent attention layer.\\n3 Method\\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).\\nSimilar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We\\ncompute the standard causal dot-product attention context within each segment. So the\\ndot-product attention computation is local in a sense that it covers a total N number of\\ntokens of the current segment with index S (N is the segment length).'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='tokens of the current segment with index S (N is the segment length).\\nThe local attention (Dai et al., 2019), however, discards the attention states of the previous\\nsegment when processing the next one. In Infini-Transformers, instead of leaving out the\\nold KV attention states, we propose to reuse them to maintain the entire context history\\nwith a compressive memory. So each attention layer of Infini-Transformers has both global\\ncompressive and local fine-grained states. We call such an efficient attention mechanism\\nInfini-attention, which is illustrated in Figure 1 and described formally in the following\\nsections.\\n3.1 Infini-attention\\nAs shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes\\nboth local and global context states and combine them for its output. Similar to multi-head\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='Preprint. Under review.\\nattention (MHA), it maintains H number of parallel compressive memory per attention\\nlayer (H is the number of attention heads) in addition to the dot-product attention and like\\nthe RNNs and MNM, it maintains a recurrent memory state to efficiently track the long\\nsequence context:\\nOs, Ms = in f ini-attention (Xs, Ms−1) (4)\\n3.1.1 Scaled Dot-product Attention\\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention\\nvariant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in\\nLLMs. The MHA’s strong capability to model context-dependent dynamic computation and\\nits conveniences of temporal masking have been leveraged extensively in the autoregressive\\ngenerative models.\\nA single head in the vanilla MHA computes its attention context Adot ∈ I RN×dvalue from\\nsequence of input segments X ∈ I RN×dmodel as follows. First, it computes attention query,\\nkey, and value states:'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='key, and value states:\\nK = XWK, V = XWV and Q = XWQ. (5)\\nHere, WK ∈ I Rdmodel ×dkey , WV ∈ I Rdmodel ×dvalue and WQ ∈ I Rdmodel ×dkey are trainable projection\\nmatrices. Then, the attention context is calculated as a weighted average of all other values\\nas\\nAdot = softmax\\n\\x12 QKT\\n√dmodel\\n\\x13\\nV. (6)\\nFor MHA, we compute H number of attention context vectors for each sequence element\\nin parallel, concatenate them along the second dimension and then finally project the\\nconcatenated vector to the model space to obtain the attention output.\\n3.1.2 Compressive Memory\\nIn Infini-attention, instead of computing new memory entries for compressive memory, we\\nreuse the query, key and value states (Q, K and V) from the dot-product attention compu-\\ntation. The state sharing and reusing between the dot-product attention and compressive\\nmemory not only enables efficient plug-in-play long-context adaptation but also speeds up'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='memory not only enables efficient plug-in-play long-context adaptation but also speeds up\\ntraining and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to\\nstore bindings of key and value states in the compressive memory and retrieve by using the\\nquery vectors.\\nWhile there are different forms of compressive memory proposed in the literature (Hop-\\nfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and\\ncomputational efficiency, in this work we parameterize the memory with an associative\\nmatrix (Schlag et al., 2020). This approach further allows us to cast the memory update\\nand retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage\\nstable training techniques from the related methods. Specially, we adopt the update rule\\nand retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and\\ncompetitive performance.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='competitive performance.\\nMemory retrieval. In Infini-attention, we retrieve new content Amem ∈ I RN×dvalue from the\\nmemory Ms−1 ∈ I Rdkey ×dvalue by using the query Q ∈ I RN×dkey as:\\nAmem = σ(Q)Ms−1\\nσ(Q)zs−1\\n. (7)\\nHere, σ and zs−1 ∈ I Rdkey are a nonlinear activation function and a normalization term,\\nrespectively. As the choice of the non-linearity and the norm method is crucial for training\\nstability, following Katharopoulos et al. (2020) we record a sum over all keys as the normal-\\nization term zs−1 and use element-wise ELU + 1 as the activation function (Clevert et al.,\\n2015).\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='Preprint. Under review.\\nModel Memory (cache) footprint Context length Memory update Memory retrieval\\nTransformer-XL (dkey + dvalue ) × H × N × l N × l Discarded Dot-product attention\\nCompressive Transformer dmodel × (c + N) × l (c × r + N) × l Discarded Dot-product attention\\nMemorizing Transformers (dkey + dvalue ) × H × N × S N × S None kNN + dot-product attention\\nRMT dmodel × p × l × 2 N × S Discarded Soft-prompt input\\nAutoCompressors dmodel × p × (m + 1) × l N × S Discarded Soft-prompt input\\nInfini-Transformers dkey × (dvalue + 1) × H × l N × S Incremental Linear attention\\nTable 1: Transformer models with segment-level memory are compared. For each model, the\\nmemory size and effective context length are defined in terms of their model parameters (N:\\ninput segment length, S: the number of segments, l: the number of layers, H: the number\\nof attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='of attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the\\nnumber of soft-prompt summary vectors and m: summary vector accumulation steps).\\nMemory update. Once the retrieval is done, we update the memory and the normalization\\nterm with the new KV entries and obtain the next states as\\nMs ← Ms−1 + σ(K)TV and zs ← zs−1 +\\nN\\n∑\\nt=1\\nσ(Kt). (8)\\nThe new memory states Ms and zs are then passed to the next segment S + 1, building in\\na recurrence in each attention layer. The right side term σ(K)TV in Eq. (8) is known as an\\nassociative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).\\nInspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),\\nwe have also incorporated it into our Infini-attention. The delta rule attempts a slightly\\nimproved memory update by first retrieving existing value entries and subtracting them\\nfrom the new values before applying the associative bindings as new update.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='from the new values before applying the associative bindings as new update.\\nMs ← Ms−1 + σ(K)T(V − σ(K)Ms−1\\nσ(K)zs−1\\n). (9)\\nThis update rule (Linear + Delta ) leaves the associative matrix unmodified if the KV binding\\nalready exists in the memory while still tracking the same normalization term as the former\\none (Linear ) for numerical stability.\\nLong-term context injection. We aggregate the local attention state Adot and memory\\nretrieved content Amem via a learned gating scalar β:\\nA = sigmoid(β) ⊙ Amem + (1 − sigmoid(β)) ⊙ Adot . (10)\\nThis adds only a single scalar value as training parameter per head while allowing a\\nlearnable trade-off between the long-term and local information flows in the model (Wu\\net al., 2022).\\nSimilar to the standard MHA, for the multi-head Infini-attention we compute H number of\\ncontext states in parallel, and concatenate and project them for the final attention output\\nO ∈ I RN×dmodel :\\nO = [A1; . . .AH]WO (11)'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='O ∈ I RN×dmodel :\\nO = [A1; . . .AH]WO (11)\\nwhere WO ∈ I RH×dvalue ×dmodel is trainable weights.\\n3.2 Memory and Effective Context Window\\nOur Infini-Transformer enables an unbounded context window with a bounded memory\\nfootprint. To illustrate this, Table 1 lists the previous segment-level memory models with\\ntheir context-memory footprint and effective context length defined in terms of model\\nparameters and input segment length. Infini-Transformer has a constant memory complexity\\nof dkey × dvalue + dkey for storing compressed context in Ms and zs for each head in single\\nlayer while for the other models, the complexity grows along with the sequence dimension\\n- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,\\n2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,\\n2022) or on the soft-prompt size for RMT (Bulatov et al., 2022) and AutoCompressors (Ge\\net al., 2023).\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='Preprint. Under review.\\nEarly \\nlayers\\nAttention heads\\nFigure 3: There are two types of heads\\nemerged in Infini-attention after training: spe-\\ncialized heads with gating score near 0 or\\n1 and mixer heads with score close to 0.5.\\nThe specialized heads either process contex-\\ntual information via the local attention mech-\\nanism or retrieve from the compressive mem-\\nory whereas the mixer heads aggregate both\\ncurrent contextual information and long-term\\nmemory content together into single output.\\nTransformer-XL computes attention over KV\\nstates cached from the last segment in addition\\nto the current states. Since this is done for each\\nlayer, Transformer-XL extends the context win-\\ndow from N to N × l tokens with an additional\\nmemory footprint of (dkey + dvalue ) × H × N × l.\\nCompressive Transformer adds a second cache\\nto Transformer-XL and stores compressed rep-\\nresentations of past segment activations. So it\\nextends the Transformer-XL’s context window'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='resentations of past segment activations. So it\\nextends the Transformer-XL’s context window\\nby c × r × l but still has a large context-memory\\ncomplexity. Taking the idea further, Memoriz-\\ning Transformers opt to store the entire KV states\\nas context for input sequences. Since the stor-\\nage becomes prohibitively expensive in this case,\\nthey restrict the contextual computation to a sin-\\ngle layer only. By utilizing a fast kNN retriever,\\nMemorizing Transformers then build a context\\nwindow covering the entire sequence history of\\nlength N × S at an increased cost of storage. Our\\nexperiments show that Infini-Transformer LM\\ncan achieve more than 100x compression rate on\\ntop of Memorizing Transformers while further\\nimproving the perplexity score.\\nRMT and AutoCompressors allow for a poten-\\ntially infinite context length since they compress\\nthe input into summary vectors and then pass them as extra soft-prompt inputs for the subse-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='the input into summary vectors and then pass them as extra soft-prompt inputs for the subse-\\nquent segments. However, in practice the success of those techniques highly depends on the\\nsize of soft-prompt vectors. Namely, it is necessary to increase the number of soft-prompt\\n(summary) vectors to achieve a better performance with AutoCompressors (Chevalier\\net al., 2023) and with that, the memory and compute complexity grow quickly resulting\\nin diminished efficiency. It was also observed in AutoCompressors (Chevalier et al., 2023)\\nthat an efficient compression objective is needed for training such prompt compression\\ntechniques (Ge et al., 2023).\\n4 Experiments\\nWe evaluated our Infini-Transformer models on benchmarks involving extremely long input\\nsequences: long-context language modeling, 1M length passkey context block retrieval\\nand 500K length book summarization tasks. For the language modeling benchmark, we'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='and 500K length book summarization tasks. For the language modeling benchmark, we\\ntrain our models from scratch while for the passkey and book summarization tasks, we\\ncontinually pre-train existing LLMs in order to highlight a plug-and-play long-context\\nadaptation capability of our approach.\\n4.1 Implementation details\\nSegment chunking. we forward-pass the entire input text a Transformer model and then\\nperform segment chunking at each Infini-attention layer - in this way, perform a minimal\\nmodification to the existing Transformer implementation. The Infini-attention layer seg-\\nments the input and process it segment by segment and concatenates back the segments to\\npass the original-length segment as output to the next layer.\\nBack-propagation through time (BPTT). Each Infini-attention layer is trained with back-\\npropagation through time (Werbos, 1988) by computing the gradient w.r.t the compressive\\nmemory states, similar to how RNNs are trained. To save memory, we perform gradient'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='memory states, similar to how RNNs are trained. To save memory, we perform gradient\\ncheckpoint when processing the sequence segment by segment.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='Preprint. Under review.\\nModel Memory size (comp.) XL cache Segment length PG19 Arxiv-math\\nTransformer-XL 50M (3.7x) 2048 2048 11.88 2.42\\nMemorizing Transformers 183M (1x) 2048 2048 11.37 2.26\\nRMT 2.5M (73x) None 2048 13.27 2.55\\nInfini-Transformer(Linear) 1.6M (114x) None 2048 9.65 2.24\\nInfini-Transformer(Linear + Delta) 1.6M (114x) None 2048 9.67 2.23\\nTable 2: Long-context language modeling results are compared in terms of average token-\\nlevel perplexity. Comp. denotes compression ratio. Infini-Transformer outperforms memo-\\nrizing transformers with memory length of 65K and achieves 114x compression ratio.\\nPosition Embeddings (PE). As shown Figure 1, we don’t use position embeddings for\\nthe key and query vectors of the compressive memory to store only global contextual\\ninformation in the long-term memory. The PEs were applied to the QK vectors only after\\nthe compressive memory reading and update.\\n4.2 Long-context Language Modeling'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='the compressive memory reading and update.\\n4.2 Long-context Language Modeling\\nWe trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019) and\\nArxiv-math (Wu et al., 2022) benchmarks. Our setup closely resembles that of Memorizing\\nTransformers (Wu et al., 2022). Namely, all our models have 12 layers and 8 attention heads\\nof dimension 128 each and FFNs with hidden layer 4096.\\nWe set the Infini-attention segment length N to 2048 for all attention layers and the input\\nsequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps\\nw.r.t its compressive memory states. For the RMT baseline, we performed several runs with\\nsummary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT\\nwith 100 summary vectors gave the best result when trained on 8196 length sequences.\\nThe main results from the language modeling experiments are summarized in Table 2. Our'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='The main results from the language modeling experiments are summarized in Table 2. Our\\nInfini-Transformer outperforms both Transformer-XL (Dai et al., 2019) and Memorizing\\nTransformers (Wu et al., 2022) baselines while maintaining 114x less memory parameters\\nthan the Memorizing Transformer model with a vector retrieval-based KV memory with\\nlength of 65K at its 9th layer.\\n100K length training. We further increased the training sequence length to 100K from\\n32K and trained the models on Arxiv-math dataset. 100K training further decreased the\\nperplexity score to 2.21 and 2.20 for Linear and Linear + Delta models.\\nGating score visualization. Figure 3 visualizes the gating score, sigmoid(β) for the compres-\\nsive memory for all attention heads in each layer. There are two types of heads emerged in\\nInfini-attention after training: specialized heads with a gating score near 0 or 1 and mixer\\nheads with a score close to 0.5. The specialized heads either process contextual information'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='heads with a score close to 0.5. The specialized heads either process contextual information\\nvia the local attention computation or retrieve from the compressive memory whereas the\\nmixer heads aggregate both current contextual information and long-term memory content\\ntogether into a single output. Interestingly, each layer has at least a single short-range\\nhead, allowing a forward-propagation of input signal up until the output layer. We also\\nZero-shot\\n32K 128K 256K 512K 1M\\nInfini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98\\nInfini-Transformer(Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97\\nFT (400 steps)\\nInfini-Transformer(Linear) 100/100/100 100/100/100 100/100/100 97/99/100 96/94/100\\nInfini-Transformer(Linear + Delta) 100/100/100 100/100/99 100/100/99 100/100/100 100/100/100\\nTable 3: Infini-Transformers solved the passkey task with up to 1M context length when\\nfine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='fine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden\\nin a different part (start/middle/end) of long inputs with lengths 32K to 1M.\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='Preprint. Under review.\\nModel Rouge-1 Rouge-2 Rouge-L Overall\\nBART 36.4 7.6 15.3 16.2\\nBART + Unlimiformer 36.8 8.3 15.7 16.9\\nPRIMERA 38.6 7.2 15.6 16.3\\nPRIMERA + Unlimiformer 37.9 8.2 16.3 17.2\\nInfini-Transformers(Linear) 37.9 8.7 17.6 18.0\\nInfini-Transformers(Linear + Delta) 40.0 8.8 17.9 18.5\\nTable 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and\\nUnlimiformer results are from Bertsch et al. (2024).\\nobserved an interleaving of long and short-term content retrievals throughout the forward\\ncomputation.\\n4.3 LLM Continual Pre-training\\nWe performed a lightweight continual pre-training for long-context adaptation of existing\\nLLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4\\ntext (Raffel et al., 2020) with length more than 4K tokens. The segment length N was set to\\n2K throughout our experiments.\\n1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infini-\\nattention and continued to pre-train on inputs with length of 4K. The model was trained for\\n30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami\\n& Jaggi, 2024).\\nThe passkey task hides a random number into a long text and asks it back at the model\\noutput. The length of the distraction text is varied by repeating a text chunk multiple times.\\nThe previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up\\nto 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.\\nWe take this challenge further and fine-tune on only 5K length inputs to test on 1M length\\nregime.\\nInput length\\nRouge overall score\\n17\\n18\\n19\\n20\\n16K 32K 64K 128K 256K 500K\\nFigure 4: Infini-Transformers obtain better\\nRouge overall scores with more book text pro-\\nvided as input.\\nTable 3 reports the token-level accuracy for'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='vided as input.\\nTable 3 reports the token-level accuracy for\\ntest subsets with input lengths ranging from\\n32K to 1M. For each test subset, we con-\\ntrolled the position of the passkey so that it\\nis either located around the beginning, mid-\\ndle or the end of the input sequence. We\\nreported both zero-shot accuracy and fine-\\ntuning accuracy. Infini-Transformers solved\\nthe task with up to 1M context length af-\\nter fine-tuning on 5K length inputs for 400\\nsteps.\\n500K length book summarization (Book-\\nSum). We further scaled our approach by\\ncontinuously pre-training a 8B LLM model\\nwith 8K input length for 30K steps. We then\\nfine-tuned on a book summarization task,\\nBookSum (Kry´sci ´nski et al., 2021) where the\\ngoal is to generate a summary of an entire\\nbook text.\\nWe set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a\\ngeneration temperature of 0.5 and top p = 0.95 and set the number of decoding steps to 1024\\nto generate a summary of each book.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='to generate a summary of each book.\\nTable 4 compares our model against the encoder-decoder models that were built particularly\\nfor the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based\\nlong-context extension (Bertsch et al., 2024). Our model outperforms the previous best\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='Preprint. Under review.\\nresults and achieves a new SOTA on BookSum by processing the entire text from book. We\\nhave also plotted the overall Rouge score on validation split of BookSum data in Figure 4.\\nThere is a clear trend showing that with more text provided as input from books, Our\\nInfini-Transformers improves its summarization performance metric.\\n5 Related Work\\nCompressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu,\\n2017a; Miconi et al., 2018), compressive memory approaches cast parameterized functions\\nas memory to store and retrieve information (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba\\net al., 2016; Munkhdalai et al., 2019). Unlike the Transformer KV memory array (Vaswani\\net al., 2017; Wu et al., 2022), which grows with input sequence length, compressive memory\\nsystems maintain a constant number of memory parameters for computational efficiency.\\nThe parameters are modified with an update rule to store information, which is then'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='The parameters are modified with an update rule to store information, which is then\\nretrieved via a memory reading mechanism (Graves et al., 2014; Sukhbaatar et al., 2015;\\nMunkhdalai & Yu, 2017b).\\nCompressed input representations can be viewed as a summary of past sequence seg-\\nments (Rae et al., 2019; Chevalier et al., 2023). Along this direction, more recent works\\nhave been utilizing a Transformer LLM itself to compress input sequence for efficient long-\\ncontext modeling (Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2023; Mu et al., 2024;\\nHwang et al., 2024). However, the previous segment-level compression methods, including\\nCompressive Transformers (Rae et al., 2019) still discard the memory entries of old segments\\nin order to free up space for the new ones, limiting their context window to the most recent\\nsegments. This is in contrast to our Infini-attention that computes incremental memory\\nupdates to a fixed amount of memory parameters in a recurrent fashion.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='updates to a fixed amount of memory parameters in a recurrent fashion.\\nLong-context continual pre-training. There is a line of work that extends the dot-product\\nattention layers and continues to train LLMs for long-context (Xiong et al., 2023; Fu et al.,\\n2024). The attention extensions include incorporating sparsity into the attention layer (Chen\\net al., 2023b; Ratner et al., 2022; Mohtashami & Jaggi, 2024) as well as manipulating the\\nposition encodings (Chen et al., 2023a; Peng et al., 2023). Although the position encoding-\\nbased methods such as position interpolation techniques (Chen et al., 2023a) can be data\\nefficient as they only adjust the positional bias in the attention layer, they are still costly for\\ninference.\\nThe attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and\\nlost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='lost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context\\nlength is longer than what was observed during training (Press et al., 2021; Kazemnejad\\net al., 2024). The proposed Infini-attention addresses those issues by enabling a segment-\\nlevel streaming computation over long sequences with a fixed local attention window. Our\\nInfini-Transformers successfully extrapolate to 1M input length regimes when trained on\\n32K and even 5K length sequences.\\nEfficient attention. The efficient attention techniques attempt to improve the efficiency of\\nthe dot-product attention with an approximation or a system-level optimization. Multiple\\ndirections have been explored for different forms of efficient attention approximation,\\nincluding sparsity-based (Child et al., 2019; Beltagy et al., 2020; Sukhbaatar et al., 2021;\\nDing et al., 2023; Xiao et al., 2024) and linear attention approximation (Shen et al., 2018;'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='Ding et al., 2023; Xiao et al., 2024) and linear attention approximation (Shen et al., 2018;\\nKatharopoulos et al., 2020; Schlag et al., 2021). Among those, the linear attention variants\\nare closely related to the associative memory matrix (Schlag et al., 2020; 2021) and the\\nmetalearned neural memory (Munkhdalai et al., 2019), where KV bindings (Smolensky,\\n1990) are stored in Fast-Weights (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016)\\nthat are modified in with respect to new contextual information. More recently, system-level\\noptimization techniques have been proposed by leveraging specific hardware architecture\\nto make the exact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='Preprint. Under review.\\n6 Conclusion\\nAn effective memory system is crucial not just for comprehending long contexts with LLMs,\\nbut also for reasoning, planning, continual adaptation for fresh knowledge, and even for\\nlearning how to learn. This work introduces a close integration of compressive memory mod-\\nule into the vanilla dot-product attention layer. This subtle but critical modification to the\\nattention layer enables LLMs to process infinitely long contexts with bounded memory and\\ncomputation resources. We show that our approach can naturally scale to a million length\\nregime of input sequences, while outperforming the baselines on long-context language\\nmodeling benchmark and book summarization tasks. We also demonstrate a promising\\nlength generalization capability of our approach. 1B model that was fine-tuned on up to 5K\\nsequence length passkey instances solved the 1M length problem.\\nAcknowledgments'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='sequence length passkey instances solved the 1M length problem.\\nAcknowledgments\\nWe would like to thank Dongseong Hwang for their help implementing efficient sequence\\nunrolling mechanism with the jax scan function. We would also like to thank Aditya Gupta,\\nKalpesh Krishna, Tu Vu and Alexandra Chronopoulou for their feedback.\\nReferences\\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\\nJimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using\\nfast weights to attend to the recent past. Advances in neural information processing systems,\\n29, 2016.\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document trans-\\nformer. arXiv preprint arXiv:2004.05150, 2020.\\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Long-\\nrange transformers with unlimited length input. Advances in Neural Information Processing\\nSystems, 36, 2024.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\\nLanguage models are few-shot learners. Advances in neural information processing systems,\\n33:1877–1901, 2020.\\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.Advances\\nin Neural Information Processing Systems, 35:11079–11091, 2022.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending con-\\ntext window of large language models via positional interpolation. arXiv preprint\\narXiv:2306.15595, 2023a.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='arXiv:2306.15595, 2023a.\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.\\nLonglora: Efficient fine-tuning of long-context large language models. arXiv preprint\\narXiv:2309.12307, 2023b.\\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\\nmachine reading. arXiv preprint arXiv:1601.06733, 2016.\\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language\\nmodels to compress contexts. arXiv preprint arXiv:2305.14788, 2023.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='Preprint. Under review.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\nDjork-Arn´e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep\\nnetwork learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\\ndinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv\\npreprint arXiv:1901.02860, 2019.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. Flashattention: Fast\\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\\nProcessing Systems, 35:16344–16359, 2022.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,\\nNanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.\\narXiv preprint arXiv:2307.02486, 2023.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='arXiv preprint arXiv:2307.02486, 2023.\\nYao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and\\nHao Peng. Data engineering for scaling language models to 128k context. arXiv preprint\\narXiv:2402.10171, 2024.\\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. arXiv preprint arXiv:2307.06945, 2023.\\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint\\narXiv:1410.5401, 2014.\\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Acceler-\\nating the science of language models. arXiv preprint arXiv:2402.00838, 2024.\\nDonald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology\\npress, 2005.\\nGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='press, 2005.\\nGeoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In\\nProceedings of the ninth annual conference of the Cognitive Science Society, pp. 177–186, 1987.\\nSepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9\\n(8):1735–1780, 1997.\\nJohn J Hopfield. Neural networks and physical systems with emergent collective computa-\\ntional abilities. Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.\\nDongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno\\nMengibar. Transformerfam: Feedback attention is working memory. arXiv preprint\\narXiv:2404.09173, 2024.\\nŁukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint\\narXiv:1511.08228, 2015.\\nPentti Kanerva. Sparse distributed memory. MIT press, 1988.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers\\nare rnns: Fast autoregressive transformers with linear attention. In International conference\\non machine learning, pp. 5156–5165. PMLR, 2020.\\nAmirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and\\nSiva Reddy. The impact of positional encoding on length generalization in transformers.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\nWojciech Kry´sci´nski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir\\nRadev. Booksum: A collection of datasets for long-form narrative summarization. arXiv\\npreprint arXiv:2105.08209, 2021.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='Preprint. Under review.\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence\\npre-training for natural language generation, translation, and comprehension. arXiv\\npreprint arXiv:1910.13461, 2019.\\nHao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for\\nnear-infinite context. arXiv preprint arXiv:2310.01889, 2023.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,\\nand Percy Liang. Lost in the middle: How language models use long contexts.Transactions\\nof the Association for Computational Linguistics, 12:157–173, 2024.\\nWolfgang Maass, Thomas Natschl¨ager, and Henry Markram. Real-time computing without\\nstable states: A new framework for neural computation based on perturbations. Neural\\ncomputation, 14(11):2531–2560, 2002.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='computation, 14(11):2531–2560, 2002.\\nThomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic\\nneural networks with backpropagation. In International Conference on Machine Learning,\\npp. 3559–3568. PMLR, 2018.\\nAmirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for\\ntransformers. Advances in Neural Information Processing Systems, 36, 2024.\\nJesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.\\nAdvances in Neural Information Processing Systems, 36, 2024.\\nTsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine\\nlearning, pp. 2554–2563. PMLR, 2017a.\\nTsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In Proceedings of the\\nconference. Association for Computational Linguistics. Meeting, volume 1, pp. 397. NIH Public\\nAccess, 2017b.\\nTsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='Tsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention\\nmodels. In Proceedings of the Seventh International Workshop on Health Text Mining and\\nInformation Analysis, pp. 69–77, 2016.\\nTsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned\\nneural memory. Advances in Neural Information Processing Systems, 32, 2019.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\\nwindow extension of large language models. arXiv preprint arXiv:2309.00071, 2023.\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,\\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling trans-\\nformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear\\nbiases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.\\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\\nunified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551,\\n2020.\\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon\\nShashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve\\nin-context learning of large language models. arXiv preprint arXiv:2212.10947, 2022.\\nImanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, J ¨urgen Schmidhuber,\\nand Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='and Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math\\nproblem solving. arXiv preprint arXiv:1910.06611, 2019.\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Preprint. Under review.\\nImanol Schlag, Tsendsuren Munkhdalai, and J ¨urgen Schmidhuber. Learning associative\\ninference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020.\\nImanol Schlag, Kazuki Irie, and J ¨urgen Schmidhuber. Linear transformers are secretly\\nfast weight programmers. In International Conference on Machine Learning, pp. 9355–9366.\\nPMLR, 2021.\\nJ ¨urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic\\nrecurrent networks. Neural Computation, 4(1):131–139, 1992.\\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear\\nmemory cost. In International Conference on Machine Learning, pp. 4596–4604. PMLR, 2018.\\nZhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient\\nattention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018.\\nPaul Smolensky. Tensor product variable binding and the representation of symbolic'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Paul Smolensky. Tensor product variable binding and the representation of symbolic\\nstructures in connectionist systems. Artificial intelligence, 46(1-2):159–216, 1990.\\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.\\nAdvances in neural information processing systems, 28, 2015.\\nSainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,\\nand Angela Fan. Not all memories are created equal: Learning to forget by expiring. In\\nInternational Conference on Machine Learning, pp. 9902–9912. PMLR, 2021.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\\ntion processing systems, 30, 2017.\\nPaul J Werbos. Generalization of backpropagation with application to a recurrent gas market\\nmodel. Neural networks, 1(4):339–356, 1988.\\nYuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing\\ntransformers. arXiv preprint arXiv:2203.08913, 2022.\\nChaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang,\\nZhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity\\nof llms for understanding extremely long sequences with training-free memory. arXiv\\npreprint arXiv:2402.04617, 2024.\\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient stream-\\ning language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramid-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramid-\\nbased masked sentence pre-training for multi-document summarization. arXiv preprint\\narXiv:2110.08499, 2021.\\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective\\nlong-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.\\nA Additional Training Details\\nFor the long-context language modeling task, we set the learning rate to 0.01 by perform-\\ning small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor opti-\\nmizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine\\ndecay. We applied gradient checkpointing after each segment to save to save memory. The\\nbatch size was set to 64. For the LLM experiments, we set the learning rate to 0.0001 during\\ncontinual pre-training and task fine-tuning.\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': '', 'keywords': '', 'moddate': '2024-08-13T00:09:01+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'leave_no_context_behind.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='Preprint. Under review.\\nB Passkey Retrieval Task\\nBelow we showed the input format of the passkey task.\\nThere is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I\\nwill quiz you about the important information there. The grass is green. The sky is blue. The sun\\nis yellow. Here we go. There and back again.(repeat x times)The pass key is9054. Remember\\nit. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.\\nThere and ack again. (repeat y times)What is the pass key? The pass key is\\n14')]\n"
     ]
    }
   ],
   "source": [
    "splits = splitter.split_documents(document)\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08691dd3-8841-4515-ac1f-04eca880e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_splits = []\n",
    "for d in splits:\n",
    "    text = d.page_content\n",
    "    if \"references\" not in text.lower() and \"bibliography\" not in text.lower():\n",
    "        cleaned_splits.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04e10a07-3362-4807-9242-7a8dda965dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original chunks: 56, After cleaning: 55\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original chunks: {len(splits)}, After cleaning: {len(cleaned_splits)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a51922-4401-43ae-be16-b415bf568dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain_google_vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ee7f00-cff6-41ad-8d5c-da2366e72d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af323155-e0e5-4469-9014-26cc2e561f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f648037-c242-4940-aded-d14c835dfb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ff229e-a688-436f-a6d8-41a03ede8b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_13412\\323160131.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80e2bcb-6c48-4dd6-8946-3e62b8ad99fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc021e36-0b0a-4b22-b23c-571948f7a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_13412\\3979189054.py:2: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(cleaned_splits, embedding, persist_directory=\"./chroma_db\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e48655cc-cc45-4893-9e84-c83a54437173",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2254dc0-54a0-4602-93bd-916e809974e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78b5641b-f784-4773-8130-e022862c7861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", max_new_tokens=256, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d19ee0a0-55a2-469c-9085-83c7550876db",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e3b00b8-1df3-4066-8ede-ae12d83c257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb97ba-8cc8-4a0f-b863-3bcc5a90605e",
   "metadata": {},
   "source": [
    "#### Questions you can ask\n",
    "\n",
    "What problem in large language models does the Leave No Context Behind paper aim to solve?\n",
    "\n",
    "What is the main contribution of the Infini-attention mechanism?\n",
    "\n",
    "How does compressive memory work in Infini-attention?\n",
    "\n",
    "What are the differences between local causal attention and long-term linear attention?\n",
    "\n",
    "How does Infini-attention achieve scalability for infinitely long inputs?\n",
    "\n",
    "What is the passkey retrieval task, and how did the model perform on it?\n",
    "\n",
    "How did the 8B parameter model perform on the 500K-token book summarization task compared to previous models?\n",
    "\n",
    "What memory efficiency gains (e.g., compression ratio) does Infini-attention provide?\n",
    "\n",
    "Why is Infini-attention considered a plug-and-play architecture?\n",
    "\n",
    "How could Infini-attention improve real-world applications of LLMs, like summarization or document analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8567eabf-394d-4136-94b2-413c0db461e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# Ask questions\n",
    "query = \"Why is Infini-attention considered a plug-and-play architecture?\"\n",
    "\n",
    "\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14fa2c71-2ef5-4def-8cd2-459688ccae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer : \n",
      " each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer : \\n\", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac6fbdfa-4faf-4957-a785-69382c153a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8d1668a6-91d4-4d8a-ae97-108751217295",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a he research assistant. \n",
    "Use the following context from the document to answer the question.\n",
    "Do not just copy text — instead, synthesize a clear and complete answer in your own words.\n",
    "If useful, combine multiple pieces of evidence.\n",
    "Always give concise but informative answers.\n",
    "\n",
    "context = {context}\n",
    "\n",
    "question = {question}\n",
    "\n",
    "answer :\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9394bb6b-2aac-452c-b6c7-f6977f28279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ab9b5ce-d20c-432b-9d74-c8b27513195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84ed5bab-066f-4112-b44d-c857af53c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is Infini-attention considered a plug-and-play architecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac19ce50-989d-484b-a196-355d7c97dc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke({'query':query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fc791568-cb30-4571-b5c0-1463bef899b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Why is Infini-attention considered a plug-and-play architecture?', 'result': 'There are two types of heads emerged in Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer heads with a score close to 0.5. The specialized heads either process contextual information via the local attention computation or retrieve from the compressive memory whereas the mixer heads aggregate both current contextual information and long-term memory content together into a single output. Interestingly, each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer. We also Zero-shot 32K 128K 256K 512K 1M Infini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98 Infini-Transformer(Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97 FT (400 steps) Infini-Transformer(Linear) 100/100/100 100/100/100 100/100/100 100/100/100 96/94/100 96/94/100 that are modified in with respect to', 'source_documents': [Document(metadata={'moddate': '2024-08-13T00:09:01+00:00', 'producer': 'pdfTeX-1.40.25', 'page': 2, 'title': '', 'total_pages': 14, 'keywords': '', 'page_label': '3', 'subject': '', 'source': 'leave_no_context_behind.pdf', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with hyperref', 'trapped': '/False', 'creationdate': '2024-08-13T00:09:01+00:00', 'author': ''}, page_content='As shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes\\nboth local and global context states and combine them for its output. Similar to multi-head\\n3'), Document(metadata={'keywords': '', 'total_pages': 14, 'producer': 'pdfTeX-1.40.25', 'trapped': '/False', 'creator': 'LaTeX with hyperref', 'title': '', 'source': 'leave_no_context_behind.pdf', 'subject': '', 'page': 6, 'moddate': '2024-08-13T00:09:01+00:00', 'creationdate': '2024-08-13T00:09:01+00:00', 'page_label': '7', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'author': ''}, page_content='Gating score visualization. Figure 3 visualizes the gating score, sigmoid(β) for the compres-\\nsive memory for all attention heads in each layer. There are two types of heads emerged in\\nInfini-attention after training: specialized heads with a gating score near 0 or 1 and mixer\\nheads with a score close to 0.5. The specialized heads either process contextual information\\nvia the local attention computation or retrieve from the compressive memory whereas the\\nmixer heads aggregate both current contextual information and long-term memory content\\ntogether into a single output. Interestingly, each layer has at least a single short-range\\nhead, allowing a forward-propagation of input signal up until the output layer. We also\\nZero-shot\\n32K 128K 256K 512K 1M\\nInfini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98\\nInfini-Transformer(Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97\\nFT (400 steps)\\nInfini-Transformer(Linear) 100/100/100 100/100/100 100/100/100 97/99/100 96/94/100'), Document(metadata={'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': '', 'page': 8, 'author': '', 'total_pages': 14, 'page_label': '9', 'moddate': '2024-08-13T00:09:01+00:00', 'producer': 'pdfTeX-1.40.25', 'subject': '', 'trapped': '/False', 'creationdate': '2024-08-13T00:09:01+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'source': 'leave_no_context_behind.pdf'}, page_content='that are modified in with respect to new contextual information. More recently, system-level\\noptimization techniques have been proposed by leveraging specific hardware architecture\\nto make the exact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).\\n9')]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "709dd3ce-87d6-45e4-b3f5-2ac449623439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are two types of heads emerged in Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer heads with a score close to 0.5. The specialized heads either process contextual information via the local attention computation or retrieve from the compressive memory whereas the mixer heads aggregate both current contextual information and long-term memory content together into a single output. Interestingly, each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer. We also Zero-shot 32K 128K 256K 512K 1M Infini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98 Infini-Transformer(Linear + Delta) 13/11/99 6/9/99 7/5/99 6/8/97 7/6/97 FT (400 steps) Infini-Transformer(Linear) 100/100/100 100/100/100 100/100/100 100/100/100 96/94/100 96/94/100 that are modified in with respect to'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccb34a-053b-4d55-ad2b-a75f72fc6688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9be19a8e-0c20-41c2-97d5-72df6f063897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, StuffDocumentsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fb959e5-e11a-4256-b0de-1c6f9b3781b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful research assistant. \n",
    "Use the following document context to answer the user’s question.\n",
    "\n",
    "Instructions:\n",
    "- Do not copy raw sentences from the text.\n",
    "- Synthesize the key ideas into a clear, concise explanation.\n",
    "- If multiple pieces of evidence are relevant, combine them.\n",
    "- If numerical results or experiments are mentioned, summarize their significance.\n",
    "\n",
    "context = {context}\n",
    "\n",
    "question = {question}\n",
    "\n",
    "answer :\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4b467aed-5132-4004-aca2-44f32094c0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e77e8934-53f6-4a35-9219-57fea7095c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_13412\\4149722537.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "82aa6e46-526f-466d-9054-7cb7bd2ea386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_13412\\3732196532.py:1: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  doc_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")\n"
     ]
    }
   ],
   "source": [
    "doc_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82a9cdf3-d491-4ab3-af35-d9e9ac7346ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_13412\\93857473.py:1: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  qa_chain = RetrievalQA(combine_documents_chain=doc_chain, retriever=retriever, return_source_documents=True)\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA(combine_documents_chain=doc_chain, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4aacf7dd-d931-4243-a69e-0a43dbd1a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is Infini-attention considered a plug-and-play architecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "67db5fc5-a86d-4e8d-9d76-d6cccc07d210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8c25159-7bc2-46e7-b778-363f900578e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infini-attention is a recurrent attention mechanism that computes both local and global context states and combine them for its output. Similar to multi-head 3 Gating score visualization. Figure 3 visualizes the gating score, sigmoid() for the compres- sive memory for all attention heads in each layer. There are two types of heads emerged in Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer heads with a score close to 0.5. The specialized heads either process contextual information via the local attention computation or retrieve from the compressive memory whereas the mixer heads aggregate both current contextual information and long-term memory content together into a single output. Interestingly, each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer. We also Zero-shot 32K 128K 256K 512K 1M Infini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98 Infini-Transformer(Linear + Delta) 13/11/\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46be924b-4cea-4ba8-abeb-b6b9ba7eaab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful research assistant. \n",
    "Use the following document context to answer the user’s question.\n",
    "\n",
    "Rules:\n",
    "- Do not copy raw sentences.\n",
    "- Summarize and rephrase into your own words.\n",
    "- If multiple ideas are present, merge them into a single clear explanation.\n",
    "- Be concise but complete.\n",
    "\n",
    "context = {context}\n",
    "\n",
    "question = {question}\n",
    "\n",
    "answer :\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e597cd7e-9e20-4cc3-902c-6806f4f454f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10915223-3757-47c0-a523-987ae1c4d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93fe2a19-5a40-428a-b6a6-56c7145828ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c59eff24-1e13-476f-a81c-cd9f5c8a5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qa_chain = RetrievalQA(combine_documents_chain=doc_chain, retriever=retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dffcc286-5cee-415c-ac0b-31d95ec9c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99df0729-f200-44ee-8a20-7dd9959b3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Why is Infini-attention considered a plug-and-play architecture?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d4e76755-9919-4ad6-89e9-15bec1f9d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ddf42ce5-a603-440e-9fbf-d4f34b7bcaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infini-attention is a recurrent attention mechanism that computes both local and global context states and combine them for its output. Similar to multi-head 3 Gating score visualization. Figure 3 visualizes the gating score, sigmoid() for the compres- sive memory for all attention heads in each layer. There are two types of heads emerged in Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer heads with a score close to 0.5. The specialized heads either process contextual information via the local attention computation or retrieve from the compressive memory whereas the mixer heads aggregate both current contextual information and long-term memory content together into a single output. Interestingly, each layer has at least a single short-range head, allowing a forward-propagation of input signal up until the output layer. We also Zero-shot 32K 128K 256K 512K 1M Infini-Transformer(Linear) 14/13/98 11/14/100 6/3/100 6/7/99 8/6/98 Infini-Transformer(Linear + Delta) 13/11/\n"
     ]
    }
   ],
   "source": [
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c3cb4d-6756-4ec1-abba-a7ae3dc21729",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
