# ğŸ’¡ Leave No Context Behind: A RAG System Powered by Infini-Attention Paper

> ğŸ§  A Retrieval-Augmented Generation (RAG) system built using LangChain and Hugging Face models to answer questions based on Googleâ€™s **"Leave No Context Behind"** paper (April 2024).

---

---

## âš™ï¸ Tech Stack

| Component       | Description                                      |
|----------------|--------------------------------------------------|
| LangChain   | Framework to build LLM-powered chains            |
| Hugging Face | Models for embedding + generation                |
| Chroma       | Vector store to persist and query embeddings     |
| Flan-T5      | Instruction-tuned text generation model          |
| Sentence Transformers | Embedding model for semantic search     |
| PDFLoader    | Extracts content from research paper (PDF)       |

---

## ğŸ§° Features

- ğŸ” **Semantic Search** over the full paper using ChromaDB
- ğŸ§  **Custom Prompt Templates** to improve generation quality
- ğŸ“ **Synthesized Answers**, not just copy-paste
- ğŸ“Š Handles both technical and architectural queries
- ğŸ”„ Modular: Easily swap in other documents, LLMs, or embeddings

---

##  Overview

Large Language Models (LLMs) like Gemini or GPT often lack access to private or domain-specific information. However, critical insights are often hidden in PDFs, documents, and research papers. This project solves that problem by building a **RAG pipeline** that:

Ingests and processes the paper â€œLeave No Context Behindâ€  
Stores semantic chunks in a vector database  
Retrieves relevant context for a given query  
Synthesizes accurate, context-aware answers using an LLM

---

## ğŸ§ª Based On:  
ğŸ“„ [Leave No Context Behind â€” Google Research Paper (April 2024)](https://arxiv.org/pdf/2404.07143.pdf)  
ğŸ“š Focus: **Infini-attention**, a plug-and-play long-context Transformer architecture  



